# import torch
# import torch.nn as nn
# import torch.nn.functional as F

# class SpatioChannelAttention(nn.Module):
#     """
#     This block first applies Channel Attention and then Spatial Attention.
#     """
#     def __init__(self, in_channels, reduction_ratio=16):
#         super(SpatioChannelAttention, self).__init__()
        
#         # Channel Attention Mechanism
#         self.channel_attention = nn.Sequential(
#             # Squeeze operation: Global Average Pooling
#             nn.AdaptiveAvgPool2d(1),
#             # Excitation operation
#             nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1, padding=0, bias=True),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(in_channels // reduction_ratio, in_channels, kernel_size=1, padding=0, bias=True),
#             nn.Sigmoid()
#         )
        
#         # Spatial Attention Mechanism
#         # The paper describes concatenating average and max pooled features along the channel dimension.
#         # This is followed by a convolution and a sigmoid function. A 7x7 kernel is a common choice for this.
#         self.spatial_attention = nn.Sequential(
#             nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, bias=False),
#             nn.Sigmoid()
#         )

#     def forward(self, x):
#         # Apply Channel Attention
#         # The channel attention map is generated by exploiting inter-channel relationships.
#         channel_att_weights = self.channel_attention(x)
#         # The output of the channel attention branch is obtained by scaling the original feature map.
#         x_channel_refined = x * channel_att_weights
        
#         # Apply Spatial Attention
#         # The spatial attention map is produced by exploiting the inter-spatial relationships among features.
#         # Global average pooling and max pooling are applied along the channel axis.
#         avg_pooled_features = torch.mean(x_channel_refined, dim=1, keepdim=True)
#         max_pooled_features, _ = torch.max(x_channel_refined, dim=1, keepdim=True)
        
#         # The results are concatenated to create a feature descriptor.
#         concatenated_features = torch.cat([avg_pooled_features, max_pooled_features], dim=1)
        
#         # This feature descriptor undergoes a convolution operation.
#         spatial_att_weights = self.spatial_attention(concatenated_features)
        
#         # The final output is obtained by rescaling the channel-refined feature set.
#         output = x_channel_refined * spatial_att_weights
        
#         return output



import torch
import torch.nn as nn
import torch.nn.functional as F

class SpatioChannelAttention(nn.Module):
   
    def __init__(self, in_channels, reduction_ratio=16):
        super(SpatioChannelAttention, self).__init__()

        # Channel Attention branch
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling
        self.channel_excitation = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction_ratio, in_channels, kernel_size=1, bias=True),
            nn.Sigmoid()
        )

        # Spatial Attention branch
        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=True)  # Output has in_channels feature maps
        self.spatial_sigmoid = nn.Sigmoid()

        # Convolution after concatenation of channel and spatial outputs
        self.combine_conv = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1, padding=0, bias=True)

    def forward(self, x):
        b, c, h, w = x.size()

        # Channel Attention branch
        avg_pool = self.avg_pool(x)  # b x c x 1 x 1
        channel_att = self.channel_excitation(avg_pool)  # b x c x 1 x 1
        channel_feature = x * channel_att  # Channel scaled feature map

        # Spatial Attention branch
        avg_out = torch.mean(x, dim=1, keepdim=True)  # b x 1 x H x W
        max_out, _ = torch.max(x, dim=1, keepdim=True)  # b x 1 x H x W
        spatial_descriptor = torch.cat([avg_out, max_out], dim=1)  # b x 2 x H x W
        spatial_att = self.spatial_sigmoid(self.spatial_conv(spatial_descriptor))  # b x c x H x W
        spatial_feature = x * spatial_att  # Spatial scaled feature map

        # Concatenate spatial and channel attention refined features
        combined = torch.cat([spatial_feature, channel_feature], dim=1)  # b x 2*c x H x W

        # Apply convolution on concatenated features
        combined_conv = self.combine_conv(combined)  # b x c x H x W

        # Add original input element-wise (residual connection)
        out = combined_conv + x
        return out


